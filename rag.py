import os
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# PDF ê²½ë¡œì™€ ë²¡í„° ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì •
PDF_PATH = "noin2.pdf"
VECTOR_DIR = "vectorstore"
embeddings = OpenAIEmbeddings()

# âœ… 1. ë²¡í„°ìŠ¤í† ì–´ ë¶ˆëŸ¬ì˜¤ê¸° ë˜ëŠ” ìƒˆë¡œ ìƒì„±
if os.path.exists(VECTOR_DIR):
    vectorstore = FAISS.load_local(VECTOR_DIR, embeddings, allow_dangerous_deserialization=True)
    print("âœ… ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ")
else:
    print("ğŸ› ï¸ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤...")
    loader = PyMuPDFLoader(PDF_PATH)
    docs = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=80)
    split_documents = text_splitter.split_documents(docs)
    vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)
    vectorstore.save_local(VECTOR_DIR)
    print("âœ… ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ ì™„ë£Œ")

# 2. ê²€ìƒ‰ê¸° ìƒì„±
retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

# 3. í”„ë¡¬í”„íŠ¸ ì •ì˜
prompt = PromptTemplate.from_template(
    """ë„ˆëŠ” ë…¸ì¸ë³µì§€ìš©êµ¬ ì „ë¬¸ ìƒë‹´ ì±—ë´‡ì´ì•¼. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ì„œ ë³µì§€ìš©êµ¬ ê´€ë ¨ ìë£Œ(context)ë¥¼ ì°¸ê³ í•´ì„œ,
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì¹œì ˆí•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ë§ˆí¬ë‹¤ìš´(Markdown) í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì„œ í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•´ì¤˜.

ë‹µë³€ ì‹œ ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¥´ì„¸ìš”:
- **í•­ëª©ë³„ë¡œ ë²ˆí˜¸ ë˜ëŠ” ë¶ˆë¦¿ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•´ì„œ ë³´ê¸° ì¢‹ê²Œ ì •ë¦¬**
- **ë³µì§€ìš©êµ¬ ëª…ì¹­, ìˆ˜ê¸‰ ì¡°ê±´ì€ ëª…í™•í•˜ê²Œ ì‘ì„±**
- **êµµì€ ê¸€ì”¨, ì¸ìš©êµ¬, ì¤„ë°”ê¿ˆ, ëª©ë¡ ë“± ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•ì„ ì‚¬ìš©**
- **ë„ˆë¬´ ê¸´ ë¬¸ì¥ë³´ë‹¤ëŠ” í•­ëª©ë³„ë¡œ ì§§ê³  ëª…ë£Œí•˜ê²Œ**
- **í™•ì‹¤í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ "í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë‹ˆ êµ­ë¯¼ê±´ê°•ë³´í—˜ê³µë‹¨ì— ë¬¸ì˜í•´ ì£¼ì„¸ìš”."ë¼ê³  ì•ˆë‚´**
- ì¡´ëŒ“ë§ë¡œ, ì–´ë¥´ì‹ ë„ ì´í•´í•  ìˆ˜ ìˆëŠ” ë§íˆ¬ë¡œ ì„¤ëª…í•´ì¤˜

#Context: 
{context}

#Question:
{question}

#Answer:"""
)

# 4. LLM ìƒì„±
llm = ChatOpenAI(model_name="gpt-4o", temperature=0)

# 5. ì²´ì¸ ìƒì„±
chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# 6. ì‚¬ìš©ì ì§ˆë¬¸ ë°˜ë³µ
print("ë³µì§€ìš©êµ¬ ì±—ë´‡ì…ë‹ˆë‹¤. ì¢…ë£Œí•˜ë ¤ë©´ 'ì¢…ë£Œ' ë˜ëŠ” 'exit'ì„ ì…ë ¥í•˜ì„¸ìš”.\n")

while True:
    user_input = input("ì§ˆë¬¸: ")
    if user_input.strip().lower() in ["ì¢…ë£Œ", "exit"]:
        print("ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break
    try:
        answer = chain.invoke(user_input.strip())
        print("ë‹µë³€:", answer, "\n")
    except Exception as e:
        print("âš ï¸ ì˜¤ë¥˜ ë°œìƒ:", e)
        print("ì£„ì†¡í•©ë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.\n")
